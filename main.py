import tempfile
import os

os.environ["TMPDIR"] = "/tmp"
tempfile.tempdir = "/tmp"

from scraper.reddit_api import get_user_data
from cleaner.clean_reddit_data import clean_reddit_data
from utils.file_saver import save_to_file  # âœ… Importing new utility

def extract_username_from_url(url: str) -> str:
    parts = url.strip("/").split("/")
    if "user" in parts:
        index = parts.index("user")
        return parts[index + 1] if index + 1 < len(parts) else None
    return None

def main():
    print("Reddit Profile Scraper")
    profile_url = input("Enter Reddit profile URL: ").strip()

    username = extract_username_from_url(profile_url)
    print("ğŸ”¥ Reached username extraction block")
    print(f"ğŸ‘¤ Username detected: {username}")

    if not username:
        print("âŒ Invalid Reddit profile URL.")
        return

    print(f"ğŸ“¥ Fetching data for u/{username}...\n")

    try:
        content = get_user_data(username)
        print("âœ… Raw data fetched successfully!\n")

        print(f"ğŸ“Š Total items before cleaning: {len(content)}")

        cleaned = clean_reddit_data(content)
        print(f"ğŸ§½ Total items after cleaning: {len(cleaned)}\n")

        print("ğŸ” Cleaned Content Preview (Top 5):\n")
        for item in cleaned[:5]:
            print(item)
            print("-" * 60)

        save_to_file(username, cleaned)  # âœ… Save to text file

    except Exception as e:
        print(f"âŒ Error occurred: {str(e)}")

    print("\nğŸ”š Script completed")

if __name__ == "__main__":
    main()
